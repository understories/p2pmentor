{
  "questId": "ai_intro",
  "version": "1",
  "track": "ai",
  "title": "Intro to AI & Machine Learning",
  "description": "Build real intuition for how neural networks learn: perceptrons, activation functions, backpropagation, CNNs, transformers, and generative models.",
  "source": "AI/ML Fundamentals",
  "estimatedDuration": "90 minutes",
  "difficulty": "beginner",
  "steps": [
    {
      "stepId": "intro",
      "type": "READ",
      "title": "From Rules to Learning",
      "description": "Why hand-coded rules fail and how machines learn from data instead.",
      "contentFile": "steps/01-intro.md",
      "duration": 10,
      "order": 1,
      "required": true,
      "conceptCard": {
        "title": "ML replaces hand-written rules with learned functions",
        "body": "Traditional programs follow explicit instructions. ML systems discover the instructions by example — given inputs and correct outputs, they find the function that maps one to the other."
      }
    },
    {
      "stepId": "neural-networks",
      "type": "READ",
      "title": "Neurons, Layers, and Activation Functions",
      "description": "How a perceptron works, what activation functions do, and how layers compose into deep networks.",
      "contentFile": "steps/02-neural-networks.md",
      "duration": 15,
      "order": 2,
      "required": true,
      "conceptCard": {
        "title": "A neuron is a weighted sum passed through a nonlinearity",
        "body": "output = activation(w₁x₁ + w₂x₂ + ... + wₙxₙ + b). Without the activation function, stacking layers would just be one big linear equation."
      }
    },
    {
      "stepId": "training",
      "type": "READ",
      "title": "Gradient Descent and Backpropagation",
      "description": "How loss functions, gradients, and the chain rule drive learning.",
      "contentFile": "steps/03-training.md",
      "duration": 15,
      "order": 3,
      "required": true,
      "conceptCard": {
        "title": "Backprop applies the chain rule layer by layer",
        "body": "The gradient of the loss with respect to any weight is computed by multiplying local gradients backward through the network — the chain rule of calculus, applied at scale."
      }
    },
    {
      "stepId": "architectures",
      "type": "READ",
      "title": "Network Architectures: CNNs, RNNs, and Beyond",
      "description": "How convolutions detect spatial patterns, how recurrence handles sequences, and why attention replaced both.",
      "contentFile": "steps/04-architectures.md",
      "duration": 10,
      "order": 4,
      "required": true,
      "conceptCard": {
        "title": "Architecture encodes inductive bias",
        "body": "CNNs assume local spatial structure. RNNs assume sequential dependence. Transformers assume everything might relate to everything — and let the data decide."
      }
    },
    {
      "stepId": "transformers",
      "type": "READ",
      "title": "The Transformer and Self-Attention",
      "description": "How queries, keys, and values compute attention, and why this architecture dominates modern AI.",
      "contentFile": "steps/05-transformers.md",
      "duration": 15,
      "order": 5,
      "required": true,
      "conceptCard": {
        "title": "Attention is a soft lookup table",
        "body": "Each token produces a query that is compared against all keys. The result is a weighted sum of values — letting the model focus on the relevant parts of the input."
      }
    },
    {
      "stepId": "generative-models",
      "type": "READ",
      "title": "Generative Models: Autoregressive, Diffusion, and GANs",
      "description": "The mechanics behind text generation, image synthesis, and adversarial training.",
      "contentFile": "steps/06-generative.md",
      "duration": 10,
      "order": 6,
      "required": true,
      "reflectionPrompt": "Pick one generative model type (autoregressive, diffusion, or GAN). In your own words, describe the core mechanism that makes it work."
    },
    {
      "stepId": "practical",
      "type": "READ",
      "title": "Training in Practice: Data, Compute, and Failure Modes",
      "description": "Overfitting, regularization, data quality, scaling laws, and what actually goes wrong.",
      "contentFile": "steps/07-practical.md",
      "duration": 10,
      "order": 7,
      "required": true,
      "conceptCard": {
        "title": "More parameters need more data, or they memorize",
        "body": "A model with 1 billion parameters and 1,000 training examples will memorize them perfectly and fail on everything else. Scaling laws describe how performance improves with data, compute, and model size together."
      }
    },
    {
      "stepId": "quiz",
      "type": "QUIZ",
      "title": "Knowledge Check",
      "description": "Test your understanding of neural networks, training, and modern architectures.",
      "duration": 10,
      "order": 8,
      "required": true,
      "quizRubricId": "ai_basics_v1"
    }
  ],
  "rubrics": {
    "ai_basics_v1": {
      "version": "1",
      "passingScore": 0.7,
      "questions": [
        {
          "id": "q1",
          "type": "multiple_choice",
          "question": "A single perceptron computes: output = activation(Σ wᵢxᵢ + b). What would happen if you removed the activation function?",
          "options": [
            "The network could not be trained",
            "Stacking layers would still just compute a linear function",
            "The network would become more powerful",
            "The outputs would always be zero"
          ],
          "correctAnswer": "Stacking layers would still just compute a linear function",
          "points": 1,
          "explanation": "Without nonlinear activation functions, composing linear layers produces another linear function. The network collapses to a single-layer linear model regardless of depth."
        },
        {
          "id": "q2",
          "type": "multiple_choice",
          "question": "During backpropagation, how is the gradient of the loss with respect to a weight in an early layer computed?",
          "options": [
            "By random sampling",
            "By multiplying local gradients backward through the chain rule",
            "By comparing the weight to the output directly",
            "By computing the loss for that layer independently"
          ],
          "correctAnswer": "By multiplying local gradients backward through the chain rule",
          "points": 1,
          "explanation": "Backpropagation applies the chain rule: ∂L/∂w = ∂L/∂a · ∂a/∂z · ∂z/∂w, where each factor is a local gradient computed at each layer."
        },
        {
          "id": "q3",
          "type": "multiple_choice",
          "question": "Why do CNNs use shared (tied) weights across spatial positions?",
          "options": [
            "To save memory",
            "Because the same feature (e.g., an edge) can appear anywhere in an image",
            "Because it makes training faster",
            "To prevent overfitting"
          ],
          "correctAnswer": "Because the same feature (e.g., an edge) can appear anywhere in an image",
          "points": 1,
          "explanation": "Weight sharing encodes translational equivariance — a vertical edge detector should fire whether the edge is at pixel (10,10) or (200,200). This is the key inductive bias of CNNs."
        },
        {
          "id": "q4",
          "type": "multiple_choice",
          "question": "In the transformer self-attention mechanism, what do the Query, Key, and Value vectors represent?",
          "options": [
            "Query = the question, Key = the database index, Value = the stored content",
            "Query = input, Key = output, Value = error",
            "Query = weight, Key = bias, Value = activation",
            "Query = gradient, Key = learning rate, Value = loss"
          ],
          "correctAnswer": "Query = the question, Key = the database index, Value = the stored content",
          "points": 1,
          "explanation": "Attention computes compatibility between Q and K (via dot product), then uses the resulting weights to aggregate V. It acts like a differentiable lookup: the query asks, keys determine relevance, values provide content."
        },
        {
          "id": "q5",
          "type": "multiple_choice",
          "question": "An autoregressive language model generates text by:",
          "options": [
            "Producing all tokens simultaneously",
            "Predicting each token conditioned on all previous tokens, one at a time",
            "Looking up text from a database",
            "Randomly selecting tokens from the vocabulary"
          ],
          "correctAnswer": "Predicting each token conditioned on all previous tokens, one at a time",
          "points": 1,
          "explanation": "Autoregressive models factor the joint probability as P(x₁)·P(x₂|x₁)·P(x₃|x₁,x₂)·... Each token is sampled from a distribution conditioned on the entire preceding context."
        },
        {
          "id": "q6",
          "type": "multiple_choice",
          "question": "What is the vanishing gradient problem?",
          "options": [
            "Gradients become too large during training",
            "Gradients shrink exponentially as they propagate through many layers, making early layers learn very slowly",
            "The model runs out of memory",
            "The loss function has no gradient"
          ],
          "correctAnswer": "Gradients shrink exponentially as they propagate through many layers, making early layers learn very slowly",
          "points": 1,
          "explanation": "When activation function derivatives are < 1 (e.g., sigmoid), multiplying them across many layers drives the gradient toward zero. ReLU, residual connections, and layer normalization were developed to address this."
        },
        {
          "id": "q7",
          "type": "multiple_choice",
          "question": "In a GAN, what does the discriminator do?",
          "options": [
            "Generates new data",
            "Classifies whether its input is real training data or generated by the generator",
            "Computes the gradient",
            "Selects training examples"
          ],
          "correctAnswer": "Classifies whether its input is real training data or generated by the generator",
          "points": 1,
          "explanation": "The discriminator is trained to distinguish real from fake, while the generator is trained to fool the discriminator. This adversarial dynamic drives the generator toward producing realistic outputs."
        }
      ]
    }
  },
  "badge": {
    "id": "ai_intro",
    "name": "AI Fundamentals",
    "description": "Completed the Intro to AI & Machine Learning track and demonstrated understanding of neural networks, training algorithms, and modern architectures."
  }
}
